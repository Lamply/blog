---
title: 模型嫁接
date: 2021-06-23
math: true
description: 炼丹偏方之一
categories: 
    - 技术经验
tags: 
    - 模型加速和压缩
---
源自 https://github.com/fxmeng/filter-grafting 

一种通过使用不同超参训练多个模型，并在训练过程中相互融合模型参数以达到减少无用卷积核数的方法，原理比较玄学，原论文发表在 CVPR 2020 上  

## 融合方法
根据每层模型参数的熵来自动调节融合权重 alpha  
首先：  
$$H(x)=\sum_i^np(x_i, x_{i+1})*log\frac{1}{p(x_i, x_{i+1})}$$  
将参数 x 根据值大小分为 n 等分，统计在 `[i, i+1)` 区间的参数数量，得到概率进而计算熵 H(x)  
权重则为：  
$$ alpha = \frac{A}{\pi}\cdot arctan(c\cdot (E(W_i^{M2})-E(W_i^{M1})))+0.5 $$
其中 A 和 c 为超参数，得出的 alpha 范围应该在 `[0.5-A/2, 0.5+A/2]`，arctan 大概是为了将输出值域限定，并用 A、c 来调节不同熵差下的输出斜率（c 越小在零点附近就越平滑，c 越大零点附近越陡峭）  

## 实际效果
[[模型改进实验-202110221125]]

在小模型上部分时候指标会变好，效果看起来差不多，可能会产生行为有所不同的模型。总之在图生成领域不算多有效的方法，实验的模型和任务下零核数少也可能是原因，但有时候有好过没有

